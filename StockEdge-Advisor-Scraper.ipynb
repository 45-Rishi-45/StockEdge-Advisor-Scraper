{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to initialize Selenium WebDriver\n",
    "def init_driver():\n",
    "    options = webdriver.ChromeOptions()\n",
    "    # Uncomment the following line for headless browsing\n",
    "    # options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver\n",
    "\n",
    "# Function to load the webpage using Selenium WebDriver\n",
    "def load_url(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Wait for the page to load completely\n",
    "    return driver.page_source\n",
    "\n",
    "# Function to scrape data from the DII CM Provisional page\n",
    "def scrape_dii_activity_cm(driver, url):\n",
    "    # Load the webpage\n",
    "    page_source = load_url(driver, url)\n",
    "\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    # Find the data table rows\n",
    "    data_rows = soup.find_all(\"ion-row\", {\"class\": \"ng-star-inserted\"})\n",
    "\n",
    "    if not data_rows:\n",
    "        print(\"No data table found. Check the selectors.\")\n",
    "        return\n",
    "\n",
    "    extracted_data = []\n",
    "\n",
    "    # Extract data row by row\n",
    "    for row in data_rows:\n",
    "        columns = row.find_all(\"ion-col\")\n",
    "        if len(columns) > 0:\n",
    "            date = columns[0].text.strip() if len(columns) > 0 else \"N/A\"\n",
    "            gross_purchase = columns[1].text.strip() if len(columns) > 1 else \"N/A\"\n",
    "            gross_sales = columns[2].text.strip() if len(columns) > 2 else \"N/A\"\n",
    "            net_purchase = columns[3].text.strip() if len(columns) > 3 else \"N/A\"\n",
    "\n",
    "            extracted_data.append([date, gross_purchase, gross_sales, net_purchase])\n",
    "\n",
    "    return extracted_data\n",
    "\n",
    "# Function to save data to a CSV file\n",
    "def save_to_csv(data, filename=\"dii_activity_cm_provisional.csv\"):\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        # Write header row\n",
    "        writer.writerow([\"Date\", \"Gross Purchase\", \"Gross Sales\", \"Net Purchase\"])\n",
    "        # Write data rows\n",
    "        writer.writerows(data)\n",
    "\n",
    "# Main function to execute the scraping\n",
    "def main():\n",
    "    # URL of the webpage to scrape\n",
    "    url = \"https://web.stockedge.com/fii-activity?section=cm-provisional&fii-dii-type=dii\"\n",
    "\n",
    "    # Initialize the Selenium WebDriver\n",
    "    driver = init_driver()\n",
    "\n",
    "    try:\n",
    "        # Scrape data from the DII CM Provisional page\n",
    "        data = scrape_dii_activity_cm(driver, url)\n",
    "\n",
    "        # Save the scraped data to a CSV file\n",
    "        save_to_csv(data)\n",
    "        print(\"Data has been successfully saved to 'dii_activity_cm_provisional.csv'\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        # Close the driver\n",
    "        driver.quit()\n",
    "\n",
    "# Entry point for script execution\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
